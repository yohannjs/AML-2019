{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful articles\n",
    "\n",
    "- [Dealing with categorical features](https://medium.com/hugo-ferreiras-blog/dealing-with-categorical-features-in-machine-learning-1bb70f07262d)\n",
    "\n",
    "- ['statsmodels' library](https://www.statsmodels.org/stable/index.html)\n",
    "\n",
    "- [some potentially useful packages](https://medium.com/activewizards-machine-learning-company/top-15-python-libraries-for-data-science-in-in-2017-ab61b4f9b4a7)\n",
    "\n",
    "- [other pot. useful packages](https://www.kdnuggets.com/2018/06/top-20-python-libraries-data-science-2018.html/2)\n",
    "\n",
    "- ['seaborn' library (visualization)](https://seaborn.pydata.org/tutorial.html)\n",
    "\n",
    "- [Nyttig eksempel](https://becominghuman.ai/linear-regression-in-python-with-pandas-scikit-learn-72574a2ec1a5)\n",
    "\n",
    "- [Eksempel med gradient boost regression pÃ¥ boston housing](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small tasks yet to be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# For configuration and jupiter\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import matplotlib\n",
    "import implicit\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# For performance evaluation\n",
    "from time import time\n",
    "\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"   # Required by implicit\n",
    "base = \"/mnt/workspace/AML-2019/Challenges/House_Pricing/challenge_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "### Fetching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoolQC          1196\n",
       "MiscFeature     1153\n",
       "Alley           1125\n",
       "Fence            973\n",
       "FireplaceQu      564\n",
       "LotFrontage      210\n",
       "GarageType        67\n",
       "GarageCond        67\n",
       "GarageYrBlt       67\n",
       "GarageFinish      67\n",
       "GarageQual        67\n",
       "BsmtExposure      33\n",
       "BsmtFinType2      33\n",
       "BsmtFinType1      32\n",
       "BsmtCond          32\n",
       "BsmtQual          32\n",
       "MasVnrArea         6\n",
       "MasVnrType         6\n",
       "Exterior2nd        0\n",
       "Exterior1st        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pricesRawDF = pd.read_csv(base + 'train.csv', keep_default_na = False)\n",
    "pricesRawDF = pd.read_csv(base + 'train.csv')\n",
    "null_values = pricesRawDF.isnull().sum().sort_values(ascending = False)\n",
    "null_values[:20]\n",
    "#print(null_values.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably many values missing in many of the columns. In the case all the categories this is because the category \"NA\" has been interpreted as NaN by the \"read_csv\" function. So first we wish to correct for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_NaN = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'LotFrontage',\n",
    "       'GarageType', 'GarageCond', 'GarageYrBlt', 'GarageFinish', 'GarageQual',\n",
    "       'BsmtExposure', 'BsmtFinType2', 'BsmtFinType1', 'BsmtCond', 'BsmtQual',\n",
    "       'MasVnrType']\n",
    "for col in columns_with_NaN:\n",
    "    pricesRawDF[col] = pricesRawDF[col].fillna('NA')\n",
    "\n",
    "pricesRawDF['MasVnrArea'] = pricesRawDF['MasVnrArea'].fillna(0)\n",
    "\n",
    "pricesRawDF.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformatting data\n",
    "In a statistical model or machine learning model, it is much easier to use numerical data rather than numerical data. For the parameters if the data set that are categorical, we can either make them numerical directly or split them into individual boolean columns by one-hot encoding. we have chosen to one-hot encode som columns and make some numerical.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Id', 1200)\n",
      "('MSSubClass', 15)\n",
      "('MSZoning', 5)\n",
      "('LotFrontage', 107)\n",
      "('LotArea', 913)\n",
      "('Street', 2)\n",
      "('Alley', 3)\n",
      "('LotShape', 4)\n",
      "('LandContour', 4)\n",
      "('Utilities', 2)\n"
     ]
    }
   ],
   "source": [
    "# To check if number of unique elements in coloumn exceeds number of types in data description\n",
    "# Only useful for columns with categorical data \n",
    "nr_column_categories = []\n",
    "\n",
    "for column in pricesRawDF:\n",
    "    nr_column_categories.append((column, pricesRawDF[column].nunique()))\n",
    "\n",
    "for elements in nr_column_categories[:10]:\n",
    "    print(elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is worth noting here is that in some of the columns containing categorical data; not all the different categories are represented. For example an element of the column \"MSSubClass\" can take 16 unique values based upon the its description in \"Data Description.rtf\". This means that transforming the values in categorical columns to numerical values, or one-hot-encoding will be a bit cumbersome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the categories we want to be numeric, to numeric values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from variables import cats_split, cats_num\n",
    "\n",
    "# The categories we want to split\n",
    "categorical_to_split = ['MSSubClass', 'MSZoning', 'Alley', 'LotConfig', 'Utilities',\n",
    "                       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n",
    "                       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
    "                       'Foundation', 'Heating', 'Electrical', 'GarageType', 'PavedDrive', \n",
    "                       'MiscFeature', 'SaleType', 'SaleCondition']\n",
    "\n",
    "# Casting the type of all columns we want to one-hot-key to strings\n",
    "for cat in categorical_to_split:\n",
    "    pricesRawDF[cat] = pricesRawDF[cat].astype(str, errors = 'ignore')\n",
    "\n",
    "# Categories to make numerical\n",
    "categorical_to_make_numerical = ['Street', 'LotShape', 'LandContour', 'LandSlope', 'CentralAir',\n",
    "                                'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n",
    "                                'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual',\n",
    "                                'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
    "                                'PoolQC', 'Fence']\n",
    "\n",
    "# Function to make desired columns numerical\n",
    "def numerize(rawDF, complete_mapping_dictionary):\n",
    "    DF = rawDF.copy()\n",
    "    errors = []\n",
    "    for mapping_dictionary in complete_mapping_dictionary:\n",
    "        # Fetching column name\n",
    "        col_name = mapping_dictionary['name']\n",
    "        # Fetching column\n",
    "        column_to_numerize = DF[col_name].copy()\n",
    "        # Creating columns of same size, and correct type\n",
    "        numerized_column = pd.Series(np.zeros([column_to_numerize.size]), dtype=np.int8, name=col_name)\n",
    "        # Resetting error_counter\n",
    "        error_counter = 0\n",
    "        # Looping through elements of column\n",
    "        for index, value in column_to_numerize.iteritems():\n",
    "            if (value in mapping_dictionary):\n",
    "                numerized_column.at[index] = mapping_dictionary[value]\n",
    "            else:\n",
    "                error_counter += 1\n",
    "        # Merging numerized column into dataframe\n",
    "        DF.drop(labels=col_name, axis=1, inplace=True)\n",
    "        DF[col_name] = numerized_column\n",
    "        # Appending errors to error vector\n",
    "        errors.append((col_name, error_counter))\n",
    "    return errors, DF\n",
    "\n",
    "# Function for one-hot-encoding a single column, given the column and its possible categories\n",
    "def split_and_filter(col_name, DF, categories):\n",
    "    # Extracting column to split\n",
    "    column_to_split = DF[col_name].copy()\n",
    "    # Setting new names for columns\n",
    "    new_col_names = [(col_name + '_' + category) for category in categories]\n",
    "    # Creating expanded DF of zeros\n",
    "    splitDF = pd.DataFrame(np.zeros([column_to_split.size,len(categories)], dtype = np.int8), columns = categories)\n",
    "    # Resetting error_counter\n",
    "    error_counter = 0\n",
    "    # Looping through series and setting correct values in new DF\n",
    "    for index, value in column_to_split.iteritems():\n",
    "        # Checking if the value is valid, i.e exists in the set of possible categories for one column\n",
    "        if (value in categories):\n",
    "            splitDF.at[index, value] = 1\n",
    "        else:\n",
    "            error_counter += 1\n",
    "    # Renaming columns of new dataframe\n",
    "    splitDF.columns = new_col_names\n",
    "    # Merging expansion of one column with full DataFrame\n",
    "    newDF = pd.merge(DF, splitDF, left_index=True, right_index=True)\n",
    "    # Dropping original column\n",
    "    newDF.drop([col_name], axis=1, inplace=True)\n",
    "    # Deleting unused dataframe and series to conserve memory\n",
    "    del splitDF\n",
    "    del column_to_split\n",
    "    return error_counter, newDF\n",
    "\n",
    "# Function to one-hot encode the entire dataframe given a dataframe and list of names for new columns\n",
    "def category_splitting(rawDF, list_of_categories):\n",
    "    first_err, newDF = split_and_filter(list_of_categories[0][0], rawDF, list_of_categories[0][1:])\n",
    "    errors = [(list_of_categories[0][0], first_err)]\n",
    "    for column_to_split in list_of_categories[1:]:\n",
    "        nth_err, newDF = split_and_filter(column_to_split[0], newDF, column_to_split[1:])\n",
    "        errors.append((column_to_split[0], nth_err))\n",
    "    return errors, newDF\n",
    "\n",
    "# Function to reformat a dataset to our desired format\n",
    "def reformat_dataset(rawDF):\n",
    "    num_errors, numerizedDF = numerize(rawDF, cats_num)\n",
    "    split_errors, reformattedDF = category_splitting(numerizedDF, cats_split)\n",
    "    errors = num_errors + split_errors\n",
    "    return errors, reformattedDF\n",
    "\n",
    "errors, rfmtDF = reformat_dataset(pricesRawDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing \n",
    "## Error processing\n",
    "Apart from missing values, we found some errors when reformatting the categorical columns. Now let as look at what errors we have got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BldgType', 106)\n",
      "('Exterior2nd', 85)\n",
      "('MasVnrType', 6)\n"
     ]
    }
   ],
   "source": [
    "for columns in errors:\n",
    "    if(columns[1] > 0):\n",
    "        print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that we have some errors. How we have classified \"errors\" in this sense, is that there has been an cell that did not contain one of the predefined elements in the \"Data description\" file. Now we inspect the values that give errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MasVnrType\n",
      "NA\n",
      "NA\n",
      "NA\n",
      "NA\n",
      "NA\n",
      "NA\n"
     ]
    }
   ],
   "source": [
    "from variables import error_detect\n",
    "\n",
    "error_values = []\n",
    "for defect_col_name in error_detect:\n",
    "    err = [defect_col_name[0]]\n",
    "    defect_col = pricesRawDF[defect_col_name[0]]\n",
    "    categories = defect_col_name[1:]\n",
    "    for index, value in defect_col.iteritems():\n",
    "        if value not in categories:\n",
    "            err.append(value)\n",
    "    error_values.append(err)\n",
    "\n",
    "# Find a way to print all the unique errors in each list-element of error_values\n",
    "col2print = error_values[4]\n",
    "for el in col2print:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the errors are due to the following reasons:\n",
    "    \n",
    "* MSZoning:  \n",
    " * The one category is called 'C' in the data description, but is called 'C (all)' in the data. \n",
    " * Here we need to change the dictionary\n",
    "* Neighborhood:  \n",
    " * 179 elements that should have been spelled as 'Names', have been misspelled as 'NAmes'. \n",
    " * Here we need to change the dictionary\n",
    "* BldgType:  \n",
    " * Spelling error: \"2fmCon\" instead of \"2FmCon\"\n",
    " * Spelling error: \"Duplex\" instead of \"Duplx\"\n",
    " * Spelling error: Many elements have been spelled as \"Twnhs\". the problem now is that it is impossible to discern whether they have ment the category \"TwnhsI\" or \"TwnhsE\"\n",
    " * Correcting function (that fixes spelling errors and combines the two \"Twnhs\" categories to one)\n",
    "* Exterior2nd:  \n",
    " * Spelling error: \"Wd Shng\" instead of \"Wd Sdng\"\n",
    " * Spelling error: \"CmentBd\" instead of \"CemntBd\"\n",
    " * Spelling error: \"Brk Cmn\" instead of \"BrkComm\"\n",
    " * Correcting function\n",
    "* MasVnrType:  \n",
    " * Six elements where this category is not applicable\n",
    " * No correction is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of errors corrected:  154\n"
     ]
    }
   ],
   "source": [
    "# Spelling-error-correcting function\n",
    "def correct_known_spelling_errors(rawDF, reformattedDF):\n",
    "    toReturnDF = reformattedDF.copy()\n",
    "    cols_to_correct = ['BldgType', 'Exterior2nd']\n",
    "    error_fix_count = 0\n",
    "    ################## BldgType\n",
    "    col = rawDF['BldgType']\n",
    "    for index, value in col.iteritems():\n",
    "        if value == '2fmCon':\n",
    "            toReturnDF.at[index, 'BldgType_2FmCon'] = 1\n",
    "            error_fix_count += 1\n",
    "        elif value == 'Duplex':\n",
    "            toReturnDF.at[index, 'BldgType_Duplx'] = 1\n",
    "            error_fix_count += 1\n",
    "     ################## Exterior2nd\n",
    "    col = rawDF['Exterior2nd']\n",
    "    for index, value in col.iteritems():\n",
    "        if value == 'Wd Shng':\n",
    "            toReturnDF.at[index, 'Exterior2nd_Wd Sdng'] = 1\n",
    "            error_fix_count += 1\n",
    "        elif value == 'CmentBd':\n",
    "            toReturnDF.at[index, 'Exterior2nd_CemntBd'] = 1\n",
    "            error_fix_count += 1\n",
    "        elif value == 'Brk Cmn':\n",
    "            toReturnDF.at[index, 'Exterior2nd_BrkComm'] = 1\n",
    "            error_fix_count += 1\n",
    "    print('Nr. of errors corrected: ', error_fix_count)\n",
    "    return toReturnDF\n",
    "\n",
    "rfmtDF = correct_known_spelling_errors(pricesRawDF, rfmtDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of errors corrected:  37\n"
     ]
    }
   ],
   "source": [
    "# Function to compensate for \"Twnhs\" spelling erorr\n",
    "def twnhs_combiner(rawDF, reformattedDF):\n",
    "    toReturnDF = reformattedDF.copy()\n",
    "    error_fix_count = 0\n",
    "    toReturnDF['BldgType_Twnhs'] = toReturnDF['BldgType_TwnhsI'] + toReturnDF['BldgType_TwnhsE']\n",
    "    toReturnDF.drop(['BldgType_TwnhsI', 'BldgType_TwnhsE'], axis=1, inplace=True)\n",
    "    BldgType_col = rawDF['BldgType']\n",
    "    for index, value in BldgType_col.iteritems():\n",
    "        if value == 'Twnhs':\n",
    "            toReturnDF.at[index, 'BldgType_Twnhs'] = 1 \n",
    "            error_fix_count += 1\n",
    "    print('Nr. of errors corrected: ', error_fix_count)\n",
    "    return toReturnDF\n",
    "\n",
    "correctedDF = twnhs_combiner(pricesRawDF, rfmtDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error processing on test-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoolQC          257\n",
       "MiscFeature     253\n",
       "Alley           244\n",
       "Fence           206\n",
       "FireplaceQu     126\n",
       "LotFrontage      49\n",
       "GarageCond       14\n",
       "GarageType       14\n",
       "GarageYrBlt      14\n",
       "GarageFinish     14\n",
       "GarageQual       14\n",
       "BsmtExposure      5\n",
       "BsmtCond          5\n",
       "BsmtQual          5\n",
       "BsmtFinType1      5\n",
       "BsmtFinType2      5\n",
       "MasVnrArea        2\n",
       "MasVnrType        2\n",
       "Electrical        1\n",
       "LotConfig         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataRawDF = pd.read_csv(base + 'test.csv')\n",
    "null_values = testDataRawDF.isnull().sum().sort_values(ascending = False)\n",
    "null_values[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have all the same errors as before, with the exception of one \"NA\" in Electrical. But, this can be treated as the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_NaN = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'LotFrontage',\n",
    "       'GarageType', 'GarageCond', 'GarageYrBlt', 'GarageFinish', 'GarageQual',\n",
    "       'BsmtExposure', 'BsmtFinType2', 'BsmtFinType1', 'BsmtCond', 'BsmtQual',\n",
    "       'MasVnrType', 'Electrical']\n",
    "for col in columns_with_NaN:\n",
    "    testDataRawDF[col] = testDataRawDF[col].fillna('NA')\n",
    "\n",
    "testDataRawDF['MasVnrArea'] = testDataRawDF['MasVnrArea'].fillna(0)\n",
    "\n",
    "testDataRawDF.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BldgType', 20)\n",
      "('Exterior2nd', 20)\n",
      "('MasVnrType', 2)\n",
      "('Electrical', 1)\n"
     ]
    }
   ],
   "source": [
    "for cat in categorical_to_split:\n",
    "    testDataRawDF[cat] = testDataRawDF[cat].astype(str, errors = 'ignore')\n",
    "\n",
    "errors, rfmtTestDF = reformat_dataset(testDataRawDF)\n",
    "\n",
    "for columns in errors:\n",
    "    if(columns[1] > 0):\n",
    "        print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of errors corrected:  34\n",
      "Nr. of errors corrected:  6\n"
     ]
    }
   ],
   "source": [
    "rfmtTestDF = correct_known_spelling_errors(testDataRawDF, rfmtTestDF)\n",
    "correctedTestDF = twnhs_combiner(testDataRawDF, rfmtTestDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctedDF['TotalSF'] = correctedDF['TotalBsmtSF'] + correctedDF['1stFlrSF'] + correctedDF['2ndFlrSF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsDF = correctedDF.copy()\n",
    "paramsDF.drop(['Id', 'SalePrice'], axis=1, inplace=True)\n",
    "priceCorr = correctedDF.corr().abs()['SalePrice']\n",
    "corrDF = paramsDF.corr(method=\"pearson\").abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 212)\n",
      "(260, 211)\n"
     ]
    }
   ],
   "source": [
    "def remove_correlated_cols(trainDF, testDF, corr_limit):\n",
    "    paramsDF = trainDF.copy()\n",
    "    toReturnTrainDF = trainDF.copy()\n",
    "    toReturnTestDF = testDF.copy()\n",
    "    paramsDF.drop(['Id', 'SalePrice'], axis=1, inplace=True)\n",
    "    priceCorr = trainDF.corr().abs()['SalePrice']\n",
    "    corrDF = paramsDF.corr(method=\"pearson\").abs()\n",
    "\n",
    "    highly_correlated_columns = np.where(np.logical_and((corrDF > corr_limit),(corrDF < 1.0)))\n",
    "    param_col_names = corrDF.columns\n",
    "    redundant_cols = []\n",
    "    correlatd_cols = []\n",
    "    for index in range(len(highly_correlated_columns[0])):\n",
    "        col_name1 = col_names[highly_correlated_columns[0][index]]\n",
    "        col_name2 = col_names[highly_correlated_columns[1][index]]\n",
    "        if (col_name1 not in redundant_cols) and (col_name2 not in redundant_cols):\n",
    "            correlation = corrDF.iloc[highly_correlated_columns[0][index]][col_name2]\n",
    "            correlatd_cols.append([col_name1, col_name2, correlation])\n",
    "            #print(col_name1, col_name2, correlation)\n",
    "            redundant_cols.append(col_name1)\n",
    "            redundant_cols.append(col_name2)\n",
    "\n",
    "    cols_to_remove = []\n",
    "    for row in correlatd_cols:\n",
    "        correlation1 = priceCorr[row[0]]\n",
    "        correlation2 = priceCorr[row[1]]\n",
    "        if correlation1 > correlation2:\n",
    "            cols_to_remove.append(row[1])\n",
    "        else:\n",
    "            cols_to_remove.append(row[0])\n",
    "    toReturnTrainDF.drop(cols_to_remove, axis=1, inplace=True)\n",
    "    toReturnTestDF.drop(cols_to_remove, axis=1, inplace=True)\n",
    "    return toReturnTrainDF, toReturnTestDF\n",
    "\n",
    "featureReducedTrainDF, featureReducedTestDF = remove_correlated_cols(correctedDF, correctedTestDF, 0.7)\n",
    "print(featureReducedTrainDF.shape)\n",
    "print(featureReducedTestDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove columns that have a low correlation with SalePrice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-333b982aa6ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;31m# Check input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "# Possible models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# ?\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "\n",
    "# Model training and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error # Metric\n",
    "\n",
    "# More complex model evaluation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "housing_params = rfmtDF.copy()\n",
    "# Put prices in sep columns\n",
    "housing_params.drop(['SalePrice','LotFrontage','MasVnrArea','GarageYrBlt'], axis=1, inplace=True)\n",
    "prices = rfmtDF['SalePrice']\n",
    "X_train, X_test, Y_train, Y_test  = train_test_split(housing_params, prices, test_size = 0.1, random_state = 9)\n",
    "\n",
    "# #############################################################################\n",
    "# Fit regression model\n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "mse = mean_squared_error(Y_test, clf.predict(X_test))\n",
    "\n",
    "# #############################################################################\n",
    "# Plot training deviance\n",
    "\n",
    "# compute test set deviance\n",
    "test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "for i, y_pred in enumerate(clf.staged_predict(X_test)):\n",
    "    test_score[i] = clf.loss_(Y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n",
    "         label='Training Set Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
    "         label='Test Set Deviance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Deviance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
