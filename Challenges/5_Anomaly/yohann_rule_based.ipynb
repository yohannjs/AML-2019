{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Notebook of task](https://github.com/DistributedSystemsGroup/Algorithmic-Machine-Learning/blob/master/Challenges/Anomaly_Detection/anomaly_detection_challenge.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a8/66fbb303236eb7e4caa63096814aa2675073f20aee95104920636af84a7e/fim-6.27.tar.gz (343kB)\n",
      "\u001b[K    100% |################################| 348kB 1.2MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: fim\n",
      "  Running setup.py bdist_wheel for fim ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /mnt/workspace/.cache/pip/wheels/5c/1c/94/b96c6b9a2eb858e26a675f86a908abfa53a593185b1c058823\n",
      "Successfully built fim\n",
      "Installing collected packages: fim\n",
      "Successfully installed fim-6.27\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pysbrl in /mnt/workspace/.local/lib/python3.5/site-packages (0.4.1)\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Package for scalable bayesian rule lists\n",
    "!pip3 install --user 'fim'\n",
    "!pip3 install --user 'pysbrl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elementary\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import matplotlib\n",
    "import implicit\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For elementary data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For scalable bayesian rule lists\n",
    "import pysbrl\n",
    "\n",
    "# Import dataframe and cast names\n",
    "from names import column_names, labels\n",
    "basepath = \"/mnt/datasets/anomaly/\"\n",
    "dataDF = pd.read_csv(basepath + 'data.csv', delimiter=\";\", header=None, names=column_names)\n",
    "pure_dataDF = dataDF.drop(labels, axis=1)\n",
    "anomaliesDF = dataDF.filter(labels, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyttige artikler om stratified shuffle split\n",
    "* [StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)\n",
    "* [Visualizing cross-validation behavior in scikit-learn](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py)\n",
    "* [User guide: cross validation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary error handling in dataDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any nan values   : False\n",
      "All values finite: True\n"
     ]
    }
   ],
   "source": [
    "anomaliesDF_with_zerNA = anomaliesDF.fillna(0) # Fill NaNs with 0s, considering them as \"not an anomaly\"\n",
    "anomaliesDF_with_negNA = anomaliesDF.fillna(-1) # Fill NaNs with -1 considering them as a separate class for the classifier.\n",
    "pure_dataDF_with_negNA = pure_dataDF.fillna(-1)\n",
    "\n",
    "X_t = pure_dataDF_with_negNA.drop('Date', axis=1)\n",
    "print('Any nan values   :', X_t.isnull().any().any())\n",
    "print('All values finite:', np.isfinite(np.array(X_t)).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal\n",
    "X_temp = X_t.drop(['CleanupOOMDumps', 'PreprocessorRestarts', 'DaemonRestarts'], axis=1)\n",
    "\n",
    "# Direct recasting\n",
    "direct_recast = ['Dumps', 'CompositeOOMDums', 'DeltaSize', 'MergeErrors', 'BlockingPhaseSec', \n",
    "                 'LargestTableSize', 'LargestPartitionSize', 'DiagnosisFiles', 'DiagnosisFilesSize', \n",
    "                 'LogSegmentChange']\n",
    "for column in direct_recast:\n",
    "    X_temp[column] = X_temp[column].astype(np.int64, errors='ignore')\n",
    "\n",
    "#Format recasting\n",
    "format_recast = ['CPU', 'PhysMEM', 'InstanceMEM', 'TablesAllocation', 'IndexServerAllocationLimit', \n",
    "                    'Disk']\n",
    "for column in format_recast:\n",
    "    X_temp[column] = 100*X_temp[column]\n",
    "    X_temp[column] = X_temp[column].astype(np.int64, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_temp.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove data corresponding to one NaN column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_temp.shape: (287031, 32) \t X.shape: (262520, 32)\n"
     ]
    }
   ],
   "source": [
    "def create_binary_classification(puredataDF, anomaliesDF, label):\n",
    "    y = anomaliesDF[label]\n",
    "    indices_nan_labels = list(y.index[y.isnull()])\n",
    "    \n",
    "    X_mod = puredataDF.drop(indices_nan_labels, axis=0)\n",
    "    y_mod = y.drop(indices_nan_labels)\n",
    "    y_mod = y_mod.astype(np.int64, errors='raise')\n",
    "    \n",
    "    X_mod = X_mod.reset_index().drop('index', axis=1)\n",
    "    y_mod = y_mod.reset_index().drop('index', axis=1)\n",
    "    return X_mod, y_mod\n",
    "\n",
    "X, y = create_binary_classification(X_temp, anomaliesDF, 'Check1')\n",
    "\n",
    "print('X_temp.shape:', X_temp.shape, '\\t', 'X.shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Check1    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified-shuffle-split function\n",
    "\n",
    "This function will split the a given dataframe X, and corresponding label-series y (only one column), into train, validation and test sets such that the distribution of the different labels is retained in the different data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# This function splits the dataset into 0.6 train, 0.2 val and 0.2 test sets ONLY SINGLE LABEL\n",
    "def train_val_test_split(X, y, seed):\n",
    "    # This generator splits the OG dataset into train and test sets\n",
    "    sss_train_test = StratifiedShuffleSplit(n_splits = 1, \n",
    "                                   test_size = 0.2, \n",
    "                                   train_size = 0.8, \n",
    "                                   random_state = seed)\n",
    "\n",
    "    # This generator splits the newly created train-set into train and validate sets\n",
    "    sss_train_val = StratifiedShuffleSplit(n_splits = 1, \n",
    "                                   test_size = 0.25, \n",
    "                                   train_size = 0.75, \n",
    "                                   random_state = seed)\n",
    "\n",
    "    for train_index, test_index in sss_train_test.split(X,y):\n",
    "        X_temp = X.iloc[train_index, :]\n",
    "        y_temp = y.iloc[train_index, :]\n",
    "        X_test = X.iloc[test_index, :]\n",
    "        y_test = y.iloc[test_index, :]\n",
    "\n",
    "    for train_index, test_index in sss_train_val.split(X_temp,y_temp):\n",
    "        X_train = X_temp.iloc[train_index, :]\n",
    "        y_train = y_temp.iloc[train_index, :]\n",
    "        X_val = X_temp.iloc[test_index, :]\n",
    "        y_val = y_temp.iloc[test_index, :]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_temp, y_temp\n",
    "\n",
    "# TEST ---------------------------------------------------------------------------------------\n",
    "seed = 42\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_train_big, y_train_big = train_val_test_split(X, y, seed)\n",
    "\n",
    "#print('####### Y TRAIN #######\\n', y_train.describe())\n",
    "#print('\\n######## Y VAL ########\\n', y_val.describe())\n",
    "#print('\\n####### Y TEST ########\\n', y_test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalable Bayesian Rule Lists\n",
    "### [github repo](https://github.com/myaooo/pysbrl)\n",
    "\n",
    "### Problems\n",
    "* WHEN USING DATAFRAME\n",
    "    * All elements after x[y == label] become NaN\n",
    "* WHEN USING NUMPY ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We dont need to convert every time, only once.\n",
      "Last time converting dataframe took 60.59 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "# We block print becuase categorical2pysbrl_data prints out a lot of uninteresting data to stdout. \n",
    "\n",
    "# Save dataframes to the format the SBRL library requires\n",
    "'''t0 = time()\n",
    "# Convert to numpy ndarray\n",
    "_X = X_train_big.values\n",
    "_y = y_train_big.values[:,0]\n",
    "\n",
    "with HiddenPrints():\n",
    "    pysbrl.utils.categorical2pysbrl_data(_X,\n",
    "                                        _y,\n",
    "                                        'X_train_big.out',\n",
    "                                        'y_train_big.label',\n",
    "                                        method='eclat',\n",
    "                                        supp=0.05,\n",
    "                                        zmin=1,\n",
    "                                        zmax=3)\n",
    "t1 = time()\n",
    "print('Converting dataframe took %.2f seconds' % (t1 - t0))'''\n",
    "print('We dont need to convert every time, only once.\\nLast time converting dataframe took 60.59 seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the SBRL based model took 31.58 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "\n",
    "# Using SBRL Library from https://github.com/myaooo/pysbrl\n",
    "rule_ids, outputs, rule_strings = pysbrl.train_sbrl(\"X_train_big.out\", \n",
    "                                                    \"y_train_big.label\", \n",
    "                                                    20.0, \n",
    "                                                    eta=2.0, \n",
    "                                                    max_iters=2000) \n",
    "                                                    #nchain=10, \n",
    "                                                    #alphas=[1,1])\n",
    "\n",
    "print('Training the SBRL based model took %.2f seconds' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{x8=0,x9=10000}\n",
      "{x2=2,x30=0}\n",
      "{x25=0,x2=0}\n",
      "{x2=1,x7=0,x8=0}\n",
      "{x2=2,x5=0,x7=0}\n",
      "{x28=1,x29=0,x7=0}\n",
      "{x21=-1,x22=-1}\n",
      "{x16=0,x23=6,x29=0}\n",
      "{x1=73,x25=0,x3=0}\n",
      "{x2=3,x6=0}\n",
      "{x2=5,x8=0}\n",
      "{x31=-1,x5=0}\n",
      "{x14=0,x2=6,x8=0}\n",
      "{x2=4,x8=0}\n",
      "{x23=1,x29=0,x5=0}\n",
      "{x16=0,x26=0}\n",
      "default\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "rules = [rule_strings[i] for i in rule_ids]\n",
    "\n",
    "for rule in rules:\n",
    "    print(rule)\n",
    "    \n",
    "print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e-01 9.00000000e-01]\n",
      " [9.96979817e-01 3.02018295e-03]\n",
      " [9.99823702e-01 1.76298438e-04]\n",
      " [9.98553485e-01 1.44651510e-03]\n",
      " [9.99150815e-01 8.49184783e-04]\n",
      " [9.65008201e-01 3.49917988e-02]\n",
      " [9.97164218e-01 2.83578242e-03]\n",
      " [9.79787234e-01 2.02127660e-02]\n",
      " [9.36708861e-01 6.32911392e-02]\n",
      " [9.96690528e-01 3.30947156e-03]\n",
      " [9.91091487e-01 8.90851344e-03]\n",
      " [9.88790104e-01 1.12098956e-02]\n",
      " [9.91901408e-01 8.09859155e-03]\n",
      " [9.95512505e-01 4.48749521e-03]\n",
      " [9.15697674e-01 8.43023256e-02]\n",
      " [9.68692022e-01 3.13079777e-02]\n",
      " [9.78089334e-01 2.19106665e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Rule | $P(Check1 = 1)$ | $P(Check1 = 1)$ |\n",
       "|:-----|:----------------|:----------------|\n",
       "| x8=0 and x9=10000 | 0.10000 | 0.90000 |\n",
       "| x2=2 and x30=0 | 0.99698 | 0.00302 |\n",
       "| x25=0 and x2=0 | 0.99982 | 0.00018 |\n",
       "| x2=1 and x7=0 and x8=0 | 0.99855 | 0.00145 |\n",
       "| x2=2 and x5=0 and x7=0 | 0.99915 | 0.00085 |\n",
       "| x28=1 and x29=0 and x7=0 | 0.96501 | 0.03499 |\n",
       "| x21=-1 and x22=-1 | 0.99716 | 0.00284 |\n",
       "| x16=0 and x23=6 and x29=0 | 0.97979 | 0.02021 |\n",
       "| x1=73 and x25=0 and x3=0 | 0.93671 | 0.06329 |\n",
       "| x2=3 and x6=0 | 0.99669 | 0.00331 |\n",
       "| x2=5 and x8=0 | 0.99109 | 0.00891 |\n",
       "| x31=-1 and x5=0 | 0.98879 | 0.01121 |\n",
       "| x14=0 and x2=6 and x8=0 | 0.99190 | 0.00810 |\n",
       "| x2=4 and x8=0 | 0.99551 | 0.00449 |\n",
       "| x23=1 and x29=0 and x5=0 | 0.91570 | 0.08430 |\n",
       "| x16=0 and x26=0 | 0.96869 | 0.03131 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def translate_output_to_text(rule_ids, outputs, rule_strings, filename):\n",
    "    rules = [rule_strings[i] for i in rule_ids]\n",
    "    split_rules = [rule[1:-1].split(',') for rule in rules[:-1]] # LAST RULE IS ONLY 'default'\n",
    "    \n",
    "    f = open(filename,'w+')\n",
    "    f.write('| Rule | $P(Check1 = 1)$ | $P(Check1 = 1)$ |\\n')\n",
    "    f.write('|:-----|:----------------|:----------------|\\n')\n",
    "    \n",
    "    separator = ' and '\n",
    "    for i in range(len(outputs[:-1])):\n",
    "        rule_to_write = separator.join(split_rules[i])\n",
    "        output = outputs[i]\n",
    "        string_to_write = '| ' + rule_to_write + ' | %.5f | %.5f |' % (output[0], output[1]) + '\\n'\n",
    "        f.write(string_to_write)\n",
    "        #print(string_to_write[:-1])\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    with open(filename, 'r') as fh:\n",
    "        content = fh.read()\n",
    "    display(Markdown(content))\n",
    "    \n",
    "translate_output_to_text(rule_ids=rule_ids, outputs=outputs, rule_strings=rule_strings, filename='created_rules.md')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
