{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Notebook of task](https://github.com/DistributedSystemsGroup/Algorithmic-Machine-Learning/blob/master/Challenges/Anomaly_Detection/anomaly_detection_challenge.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a8/66fbb303236eb7e4caa63096814aa2675073f20aee95104920636af84a7e/fim-6.27.tar.gz (343kB)\n",
      "\u001b[K    100% |################################| 348kB 1.2MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: fim\n",
      "  Running setup.py bdist_wheel for fim ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /mnt/workspace/.cache/pip/wheels/5c/1c/94/b96c6b9a2eb858e26a675f86a908abfa53a593185b1c058823\n",
      "Successfully built fim\n",
      "Installing collected packages: fim\n",
      "Successfully installed fim-6.27\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pysbrl in /mnt/workspace/.local/lib/python3.5/site-packages (0.4.1)\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Package for scalable bayesian rule lists\n",
    "!pip3 install --user 'fim'\n",
    "!pip3 install --user 'pysbrl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elementary\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import matplotlib\n",
    "import implicit\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For elementary data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For scalable bayesian rule lists\n",
    "import pysbrl\n",
    "\n",
    "# Import dataframe and cast names\n",
    "from names import column_names, labels\n",
    "basepath = \"/mnt/datasets/anomaly/\"\n",
    "dataDF = pd.read_csv(basepath + 'data.csv', delimiter=\";\", header=None, names=column_names)\n",
    "pure_dataDF = dataDF.drop(labels, axis=1)\n",
    "anomaliesDF = dataDF.filter(labels, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyttige artikler om stratified shuffle split\n",
    "* [StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)\n",
    "* [Visualizing cross-validation behavior in scikit-learn](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py)\n",
    "* [User guide: cross validation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary error handling in dataDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any nan values   : False\n",
      "All values finite: True\n"
     ]
    }
   ],
   "source": [
    "anomaliesDF_with_zerNA = anomaliesDF.fillna(0) # Fill NaNs with 0s, considering them as \"not an anomaly\"\n",
    "anomaliesDF_with_negNA = anomaliesDF.fillna(-1) # Fill NaNs with -1 considering them as a separate class for the classifier.\n",
    "pure_dataDF_with_negNA = pure_dataDF.fillna(-1)\n",
    "\n",
    "X_t = pure_dataDF_with_negNA.drop('Date', axis=1)\n",
    "print('Any nan values   :', X_t.isnull().any().any())\n",
    "print('All values finite:', np.isfinite(np.array(X_t)).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal\n",
    "X_temp = X_t.drop(['CleanupOOMDumps', 'PreprocessorRestarts', 'DaemonRestarts'], axis=1)\n",
    "\n",
    "# Direct recasting\n",
    "direct_recast = ['Dumps', 'CompositeOOMDums', 'DeltaSize', 'MergeErrors', 'BlockingPhaseSec', \n",
    "                 'LargestTableSize', 'LargestPartitionSize', 'DiagnosisFiles', 'DiagnosisFilesSize', \n",
    "                 'LogSegmentChange']\n",
    "for column in direct_recast:\n",
    "    X_temp[column] = X_temp[column].astype(np.int64, errors='ignore')\n",
    "\n",
    "#Format recasting\n",
    "format_recast = ['CPU', 'PhysMEM', 'InstanceMEM', 'TablesAllocation', 'IndexServerAllocationLimit', \n",
    "                    'Disk']\n",
    "for column in format_recast:\n",
    "    X_temp[column] = 100*X_temp[column]\n",
    "    X_temp[column] = X_temp[column].astype(np.int64, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_temp.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove data corresponding to one NaN column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_temp.shape: (287031, 32) \t X.shape: (262520, 32)\n"
     ]
    }
   ],
   "source": [
    "def create_binary_classification(puredataDF, anomaliesDF, label):\n",
    "    y = anomaliesDF[label]\n",
    "    indices_nan_labels = list(y.index[y.isnull()])\n",
    "    \n",
    "    X_mod = puredataDF.drop(indices_nan_labels, axis=0)\n",
    "    y_mod = y.drop(indices_nan_labels)\n",
    "    y_mod = y_mod.astype(np.int64, errors='raise')\n",
    "    \n",
    "    X_mod = X_mod.reset_index().drop('index', axis=1)\n",
    "    y_mod = y_mod.reset_index().drop('index', axis=1)\n",
    "    return X_mod, y_mod\n",
    "\n",
    "X, y = create_binary_classification(X_temp, anomaliesDF, 'Check1')\n",
    "\n",
    "print('X_temp.shape:', X_temp.shape, '\\t', 'X.shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Check1    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified-shuffle-split function\n",
    "\n",
    "This function will split the a given dataframe X, and corresponding label-series y (only one column), into train, validation and test sets such that the distribution of the different labels is retained in the different data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# This function splits the dataset into 0.6 train, 0.2 val and 0.2 test sets ONLY SINGLE LABEL\n",
    "def train_val_test_split(X, y, seed):\n",
    "    # This generator splits the OG dataset into train and test sets\n",
    "    sss_train_test = StratifiedShuffleSplit(n_splits = 1, \n",
    "                                   test_size = 0.2, \n",
    "                                   train_size = 0.8, \n",
    "                                   random_state = seed)\n",
    "\n",
    "    # This generator splits the newly created train-set into train and validate sets\n",
    "    sss_train_val = StratifiedShuffleSplit(n_splits = 1, \n",
    "                                   test_size = 0.25, \n",
    "                                   train_size = 0.75, \n",
    "                                   random_state = seed)\n",
    "\n",
    "    for train_index, test_index in sss_train_test.split(X,y):\n",
    "        X_temp = X.iloc[train_index, :]\n",
    "        y_temp = y.iloc[train_index, :]\n",
    "        X_test = X.iloc[test_index, :]\n",
    "        y_test = y.iloc[test_index, :]\n",
    "\n",
    "    for train_index, test_index in sss_train_val.split(X_temp,y_temp):\n",
    "        X_train = X_temp.iloc[train_index, :]\n",
    "        y_train = y_temp.iloc[train_index, :]\n",
    "        X_val = X_temp.iloc[test_index, :]\n",
    "        y_val = y_temp.iloc[test_index, :]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_temp, y_temp\n",
    "\n",
    "# TEST ---------------------------------------------------------------------------------------\n",
    "seed = 42\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_train_big, y_train_big = train_val_test_split(X, y, seed)\n",
    "\n",
    "#print('####### Y TRAIN #######\\n', y_train.describe())\n",
    "#print('\\n######## Y VAL ########\\n', y_val.describe())\n",
    "#print('\\n####### Y TEST ########\\n', y_test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalable Bayesian Rule Lists\n",
    "### [github repo](https://github.com/myaooo/pysbrl)\n",
    "\n",
    "### Problems\n",
    "* WHEN USING DATAFRAME\n",
    "    * All elements after x[y == label] become NaN\n",
    "* WHEN USING NUMPY ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting dataframe took 61.00 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "# We block print becuase categorical2pysbrl_data prints out a lot of uninteresting data to stdout. \n",
    "\n",
    "# Save dataframes to the format the SBRL library requires\n",
    "t0 = time()\n",
    "# Convert to numpy ndarray\n",
    "_X = X_train_big.values\n",
    "_y = y_train_big.values[:,0]\n",
    "\n",
    "with HiddenPrints():\n",
    "    pysbrl.utils.categorical2pysbrl_data(_X,\n",
    "                                        _y,\n",
    "                                        'X_train_big.out',\n",
    "                                        'y_train_big.label',\n",
    "                                        method='eclat',\n",
    "                                        supp=0.05,\n",
    "                                        zmin=1,\n",
    "                                        zmax=3)\n",
    "t1 = time()\n",
    "print('Converting dataframe took %.2f seconds' % (t1 - t0))\n",
    "# print('We dont need to convert every time, only once.\\nLast time converting dataframe took 61.00 seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the SBRL based model took 31.40 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "# Using SBRL Library from https://github.com/myaooo/pysbrl\n",
    "rule_ids, outputs, rule_strings = pysbrl.train_sbrl(\"X_train_big.out\", \n",
    "                                                    \"y_train_big.label\", \n",
    "                                                    20.0, \n",
    "                                                    eta=2.0, \n",
    "                                                    max_iters=2000) \n",
    "                                                    #nchain=10, \n",
    "                                                    #alphas=[1,1])\n",
    "\n",
    "print('Training the SBRL based model took %.2f seconds' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{x8=0,x9=10000}\n",
      "{x2=2,x30=0}\n",
      "{x25=0,x2=0}\n",
      "{x2=1,x7=0,x8=0}\n",
      "{x2=2,x5=0,x7=0}\n",
      "{x28=1,x29=0,x7=0}\n",
      "{x21=-1,x22=-1}\n",
      "{x16=0,x23=6,x29=0}\n",
      "{x1=73,x25=0,x3=0}\n",
      "{x2=3,x6=0}\n",
      "{x2=5,x8=0}\n",
      "{x31=-1,x5=0}\n",
      "{x14=0,x2=6,x8=0}\n",
      "{x2=4,x8=0}\n",
      "{x23=1,x29=0,x5=0}\n",
      "{x16=0,x26=0}\n",
      "default\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "rules = [rule_strings[i] for i in rule_ids]\n",
    "\n",
    "for rule in rules:\n",
    "    print(rule)\n",
    "    \n",
    "print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e-01 9.00000000e-01]\n",
      " [9.96979817e-01 3.02018295e-03]\n",
      " [9.99823702e-01 1.76298438e-04]\n",
      " [9.98553485e-01 1.44651510e-03]\n",
      " [9.99150815e-01 8.49184783e-04]\n",
      " [9.65008201e-01 3.49917988e-02]\n",
      " [9.97164218e-01 2.83578242e-03]\n",
      " [9.79787234e-01 2.02127660e-02]\n",
      " [9.36708861e-01 6.32911392e-02]\n",
      " [9.96690528e-01 3.30947156e-03]\n",
      " [9.91091487e-01 8.90851344e-03]\n",
      " [9.88790104e-01 1.12098956e-02]\n",
      " [9.91901408e-01 8.09859155e-03]\n",
      " [9.95512505e-01 4.48749521e-03]\n",
      " [9.15697674e-01 8.43023256e-02]\n",
      " [9.68692022e-01 3.13079777e-02]\n",
      " [9.78089334e-01 2.19106665e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Rule | $P(Check1 = 0)$ | $P(Check1 = 1)$ |\n",
       "|:-----|:----------------|:----------------|\n",
       "| MinDailyNumberOfSuccessfulDataBackups=1 and NameServerRestarts=0 and CPU=10000 | 0.10256 | 0.89744 |\n",
       "| DaysWithFailedfulLogBackups=0 and HighPriorityAlerts=5 | 0.99012 | 0.00988 |\n",
       "| HighPriorityAlerts=0 and NameServerRestarts=0 | 0.99982 | 0.00018 |\n",
       "| BlockingPhaseSec=2 and Dumps=0 and StatisticsServerRestarts=0 | 0.99938 | 0.00062 |\n",
       "| DaysWithSuccessfulDataBackups=7 and HighPriorityAlerts=3 and NameServerRestarts=0 | 0.99571 | 0.00429 |\n",
       "| DaysWithSuccessfulLogBackups=11 and MinDailyNumberOfSuccessfulDataBackups=1 and StatisticsServerRestarts=0 | 0.98615 | 0.01385 |\n",
       "| HighPriorityAlerts=1 and IndexServerRestarts=0 | 0.99884 | 0.00116 |\n",
       "| HighPriorityAlerts=2 | 0.99748 | 0.00252 |\n",
       "| MaxDailyNumberOfFailedDataBackups=0 and CPU=10000 | 0.14286 | 0.85714 |\n",
       "| MinDailyNumberOfSuccessfulLogBackups=1 and MaxDailyNumberOfFailedLogBackups=0 | 0.96375 | 0.03625 |\n",
       "| MergeErrors=1 and HighPriorityAlerts=3 and XSEngineRestarts=0 | 0.99814 | 0.00186 |\n",
       "| SystemID=73 and DaysWithFailedfulLogBackups=0 | 0.91045 | 0.08955 |\n",
       "| HighPriorityAlerts=4 | 0.99542 | 0.00458 |\n",
       "| ColumnUnloads=0 and MaxDailyNumberOfFailedDataBackups=1 and IndexServerRestarts=0 | 0.99454 | 0.00546 |\n",
       "| DiagnosisFiles=-1 and DiagnosisFilesSize=-1 | 0.99524 | 0.00476 |\n",
       "| DaysWithSuccessfulDataBackups=6 and MaxDailyNumberOfFailedLogBackups=0 and StatisticsServerRestarts=0 | 0.97845 | 0.02155 |\n",
       "| HighPriorityAlerts=3 and CompositeOOMDums=0 | 0.99581 | 0.00419 |\n",
       "| HighPriorityAlerts=5 and Dumps=0 | 0.99084 | 0.00916 |\n",
       "| ColumnUnloads=0 and HighPriorityAlerts=6 and StatisticsServerRestarts=0 | 0.99028 | 0.00972 |\n",
       "| DaysWithSuccessfulDataBackups=1 and DaysWithFailedDataBackups=0 and Dumps=0 | 0.91124 | 0.08876 |\n",
       "| BlockingPhaseSec=-1 and DaysWithSuccessfulDataBackups=7 | 0.98236 | 0.01764 |\n",
       "| Default | 0.97377 | 0.02623 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def translate_output_to_text(rule_ids, outputs, rule_strings, filename, dataDF, label):\n",
    "    rules = [rule_strings[i] for i in rule_ids]\n",
    "    split_rules = [rule[1:-1].split(',') for rule in rules[:-1]] # LAST RULE IS ONLY 'default'\n",
    "    \n",
    "    rules_with_column_names = []\n",
    "    for num_rule in split_rules:\n",
    "        rule_with_column_names = []\n",
    "        for sub_rule in num_rule:\n",
    "            col_number = int(sub_rule.split('=')[0][1:])\n",
    "            col_name = dataDF.columns[col_number]\n",
    "            new_sub_rule = col_name + '=' + sub_rule.split('=')[1]\n",
    "            rule_with_column_names.append(new_sub_rule)\n",
    "        rules_with_column_names.append(rule_with_column_names)\n",
    "    \n",
    "    \n",
    "    f = open(filename,'w+')\n",
    "    f.write('| Rule | $P(%s = 0)$ | $P(%s = 1)$ |\\n' % (label, label))\n",
    "    f.write('|:-----|:----------------|:----------------|\\n')\n",
    "    \n",
    "    separator = ' and '\n",
    "    for i in range(len(outputs[:-1])):\n",
    "        rule_to_write = separator.join(rules_with_column_names[i])\n",
    "        output = outputs[i]\n",
    "        string_to_write = '| ' + rule_to_write + ' | %.5f | %.5f |' % (output[0], output[1]) + '\\n'\n",
    "        f.write(string_to_write)\n",
    "        #print(string_to_write[:-1])\n",
    "    \n",
    "    default_prob = outputs[-1]\n",
    "    f.write('| Default | %.5f | %.5f |\\n' % (default_prob[0], default_prob[1]))\n",
    "    f.close()\n",
    "    \n",
    "    with open(filename, 'r') as fh:\n",
    "        content = fh.read()\n",
    "    display(Markdown(content))\n",
    "    \n",
    "translate_output_to_text(rule_ids=rule_ids, \n",
    "                         outputs=outputs, \n",
    "                         rule_strings=rule_strings, \n",
    "                         filename='created_rules.md', \n",
    "                         dataDF=X_train_big,\n",
    "                         label='Check1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x27=1', 'x6=0', 'x9=10000']\n",
      "['x26=0', 'x2=5']\n",
      "['x2=0', 'x6=0']\n",
      "['x17=2', 'x3=0', 'x8=0']\n",
      "['x23=7', 'x2=3', 'x6=0']\n",
      "['x24=11', 'x27=1', 'x8=0']\n",
      "['x2=1', 'x5=0']\n",
      "['x2=2']\n",
      "['x29=0', 'x9=10000']\n",
      "['x28=1', 'x30=0']\n",
      "['x16=1', 'x2=3', 'x7=0']\n",
      "['x1=73', 'x26=0']\n",
      "['x2=4']\n",
      "['x14=0', 'x29=1', 'x5=0']\n",
      "['x21=-1', 'x22=-1']\n",
      "['x23=6', 'x30=0', 'x8=0']\n",
      "['x2=3', 'x4=0']\n",
      "['x2=5', 'x3=0']\n",
      "['x14=0', 'x2=6', 'x8=0']\n",
      "['x23=1', 'x25=0', 'x3=0']\n",
      "['x17=-1', 'x23=7']\n",
      "['MinDailyNumberOfSuccessfulDataBackups=1', 'NameServerRestarts=0', 'CPU=10000']\n",
      "['DaysWithFailedfulLogBackups=0', 'HighPriorityAlerts=5']\n",
      "['HighPriorityAlerts=0', 'NameServerRestarts=0']\n",
      "['BlockingPhaseSec=2', 'Dumps=0', 'StatisticsServerRestarts=0']\n",
      "['DaysWithSuccessfulDataBackups=7', 'HighPriorityAlerts=3', 'NameServerRestarts=0']\n",
      "['DaysWithSuccessfulLogBackups=11', 'MinDailyNumberOfSuccessfulDataBackups=1', 'StatisticsServerRestarts=0']\n",
      "['HighPriorityAlerts=1', 'IndexServerRestarts=0']\n",
      "['HighPriorityAlerts=2']\n",
      "['MaxDailyNumberOfFailedDataBackups=0', 'CPU=10000']\n",
      "['MinDailyNumberOfSuccessfulLogBackups=1', 'MaxDailyNumberOfFailedLogBackups=0']\n",
      "['MergeErrors=1', 'HighPriorityAlerts=3', 'XSEngineRestarts=0']\n",
      "['SystemID=73', 'DaysWithFailedfulLogBackups=0']\n",
      "['HighPriorityAlerts=4']\n",
      "['ColumnUnloads=0', 'MaxDailyNumberOfFailedDataBackups=1', 'IndexServerRestarts=0']\n",
      "['DiagnosisFiles=-1', 'DiagnosisFilesSize=-1']\n",
      "['DaysWithSuccessfulDataBackups=6', 'MaxDailyNumberOfFailedLogBackups=0', 'StatisticsServerRestarts=0']\n",
      "['HighPriorityAlerts=3', 'CompositeOOMDums=0']\n",
      "['HighPriorityAlerts=5', 'Dumps=0']\n",
      "['ColumnUnloads=0', 'HighPriorityAlerts=6', 'StatisticsServerRestarts=0']\n",
      "['DaysWithSuccessfulDataBackups=1', 'DaysWithFailedDataBackups=0', 'Dumps=0']\n",
      "['BlockingPhaseSec=-1', 'DaysWithSuccessfulDataBackups=7']\n"
     ]
    }
   ],
   "source": [
    "def test_function(rule_ids, outputs, rule_strings, dataDF, label):\n",
    "    rules = [rule_strings[i] for i in rule_ids]\n",
    "    split_rules = [rule[1:-1].split(',') for rule in rules[:-1]] # LAST RULE IS ONLY 'default'\n",
    "    \n",
    "    rules_with_column_names = []\n",
    "    for num_rule in split_rules:\n",
    "        print(num_rule)\n",
    "        rule_with_column_names = []\n",
    "        for sub_rule in num_rule:\n",
    "            col_number = int(sub_rule.split('=')[0][1:])\n",
    "            col_name = dataDF.columns[col_number]\n",
    "            new_sub_rule = col_name + '=' + sub_rule.split('=')[1]\n",
    "            rule_with_column_names.append(new_sub_rule)\n",
    "        rules_with_column_names.append(rule_with_column_names)\n",
    "    \n",
    "            \n",
    "\n",
    "test_function(rule_ids=rule_ids, outputs=outputs, rule_strings=rule_strings, dataDF=X_train_big, label='Check1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
