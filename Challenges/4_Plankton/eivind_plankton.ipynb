{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import matplotlib\n",
    "import implicit\n",
    "import warnings\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# For prediction\n",
    "from tqdm import tqdm\n",
    "\n",
    "from extract_zip import extract_zip_to_memory # function for decompressing zip in memory\n",
    "from PIL import Image, ImageOps # image handling\n",
    "from io import BytesIO # image handling\n",
    "\n",
    "# for the CNN\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "basepath = \"/mnt/datasets/plankton/flowcam/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this Notebook we explore a computer vision problem. We are tasked with classifying the taxonomy of plankton from images using deep neural networks. Our main approach is based on processing images with a Convolutional Neural Network with Keras. We will compare our approach with classification on carefully (but manually) engineered features pre-computed on each image object.\n",
    "\n",
    "Throughout the task we will learn and explore common problems in computer vision. We will process images, augment them to increase the amount of data and look at computational cost when doing CNNs. The goal of this task is not to achieve the highest possible accuracy, but rather explore and learn about image classification. \n",
    "\n",
    "The data was prepared in cooperation with the Laboratoire d’Océanographie de Villefranche, jointly run by Sorbonne Université and CNRS. Plankton images were acquired in the bay of Villefranche, weekly since 2013 and manually engineered features were computed on each imaged object.\n",
    "\n",
    "## Summary\n",
    "Using the manually engineered features in a Random Forest Classifier gave the best result with a F1 Macro Score of 53%. This is significantly better than the performance of the CNN. Still there is a lot of performance to gain in the CNN as we choose options which reduced our computational expenses. Increasing the size of the input to the CNN should give more information to the CNN, which could give a better classification. Inreasing the complexity of the CNN while running more epochs could also increase the score. An architecture like VGG16 have handled similar problems very well. \n",
    "\n",
    "We have handled complications like skewness in input data, loosing information in image processing and handling of increasing computational costs. As it was specified in the task that the purpose of this challenge was not to simply find the best-performing model, but to explore the dificulties that come with computer vision, we are content with our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 243611/243611 [00:23<00:00, 10364.51it/s]\n"
     ]
    }
   ],
   "source": [
    "labelsDF = pd.read_csv(basepath + 'meta.csv')\n",
    "imgFiles = extract_zip_to_memory(basepath + \"imgs.zip\")\n",
    "\n",
    "labelsDF['objid'] = labelsDF['objid'].astype(np.int64, errors='ignore')\n",
    "labelsDF['level1'] = labelsDF['level1'].fillna('No_level1_name')\n",
    "labelsDF['level2'] = labelsDF['level2'].fillna('No_level2_name')\n",
    "\n",
    "img_keys = list(imgFiles.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Anne has 481 object IDs.\n",
      "Label: Aste has 117 object IDs.\n",
      "Label: Bact has 12 object IDs.\n",
      "Label: Cera has 186 object IDs.\n",
      "Label: Chae has 2105 object IDs.\n",
      "Label: Codo has 845 object IDs.\n",
      "Label: Codo has 2888 object IDs.\n",
      "Label: Cope has 5141 object IDs.\n",
      "Label: Cosc has 334 object IDs.\n",
      "Label: Cytt has 100 object IDs.\n",
      "Label: Dict has 549 object IDs.\n",
      "Label: Dino has 525 object IDs.\n",
      "Label: Hemi has 670 object IDs.\n",
      "Label: Lith has 68 object IDs.\n",
      "Label: Neoc has 14014 object IDs.\n",
      "Label: No_l has 1003 object IDs.\n",
      "Label: Odon has 131 object IDs.\n",
      "Label: Pleu has 191 object IDs.\n",
      "Label: Prot has 2256 object IDs.\n",
      "Label: Reta has 257 object IDs.\n",
      "Label: Rhab has 367 object IDs.\n",
      "Label: Rhiz has 2160 object IDs.\n",
      "Label: Sten has 357 object IDs.\n",
      "Label: Thal has 5117 object IDs.\n",
      "Label: Tint has 2227 object IDs.\n",
      "Label: Unde has 710 object IDs.\n",
      "Label: Xyst has 37 object IDs.\n",
      "Label: arte has 1849 object IDs.\n",
      "Label: badf has 7848 object IDs.\n",
      "Label: cent has 145 object IDs.\n",
      "Label: chai has 751 object IDs.\n",
      "Label: detr has 138439 object IDs.\n",
      "Label: egg  has 685 object IDs.\n",
      "Label: fece has 26936 object IDs.\n",
      "Label: mult has 3261 object IDs.\n",
      "Label: naup has 9293 object IDs.\n",
      "Label: poll has 1821 object IDs.\n",
      "Label: rods has 4044 object IDs.\n",
      "Label: silk has 5629 object IDs.\n",
      "Label: temp has 61 object IDs.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "number_unique_names = labelsDF.groupby('level2', as_index=True)['id'].count()\n",
    "unique_names = list(number_unique_names.index)\n",
    "original_objid_dict = {}\n",
    "labels_to_multiply = []\n",
    "labels_not_to_multiply = []\n",
    "\n",
    "for name in unique_names:\n",
    "    objid_labels = list(labelsDF.loc[labelsDF['level2'] == name, 'objid'])\n",
    "    original_objid_dict.update({name : objid_labels})\n",
    "    print('Label:', name[:4], 'has', len(objid_labels), 'object IDs.')\n",
    "    # Find labels that we want to generate more data within\n",
    "    if len(objid_labels) < 1000:\n",
    "        labels_to_multiply.append(name)\n",
    "    else:\n",
    "        labels_not_to_multiply.append(name)\n",
    "\n",
    "# first split keys\n",
    "def train_test_val_split(img_dict, train_size=0.6, test_size=0.2):\n",
    "    train_objids = []\n",
    "    test_objids = [] \n",
    "    val_objids = []\n",
    "    for label in unique_names:\n",
    "        object_ids = img_dict[label]\n",
    "        number_train = int(len(object_ids)*train_size)\n",
    "        number_test = int(len(object_ids)*test_size)\n",
    "        train_objids += object_ids[:number_train]\n",
    "        test_objids += object_ids[number_train:(number_train + number_test)]\n",
    "        val_objids += object_ids[(number_train + number_test):]\n",
    "    \n",
    "    train_objids = random.sample(train_objids, len(train_objids))\n",
    "    test_objids = random.sample(test_objids, len(test_objids))\n",
    "    val_objids = random.sample(val_objids, len(val_objids))\n",
    "    return train_objids, test_objids, val_objids\n",
    "\n",
    "def autoloader(list_of_object_ids, imagefiles):\n",
    "    X = np.array([imagefiles[object_id][0] for object_id in list_of_object_ids])\n",
    "    y = np.array([imagefiles[object_id][1] for object_id in list_of_object_ids])\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    # one-hot encode target column\n",
    "    y = to_categorical(y)\n",
    "\n",
    "    # Reshape input data to account for images only being greyscale\n",
    "    X = X.reshape(len(list_of_object_ids), 40, 40, 1)\n",
    "    return X, y\n",
    "\n",
    "#split ids to train, test and val\n",
    "train_objids, test_objids, val_objids = train_test_val_split(original_objid_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip(key):\n",
    "    return int(key[5:-4])\n",
    "\n",
    "def key_to_objid(list_of_keys):\n",
    "    return [strip(key) for key in list_of_keys]\n",
    "\n",
    "def reverse_strip(objid):\n",
    "    return 'imgs/' + str(objid) + '.jpg'\n",
    "\n",
    "def objid_to_key(list_of_objids):\n",
    "    return [reverse_strip(objid) for objid in list_of_objids]\n",
    "\n",
    "# Functions to preprocess images\n",
    "def image_processing(key):\n",
    "    desired_size = 972 # max of width and height in dataset\n",
    "    im_orginal = Image.open(imgFiles[key])\n",
    "    old_size = im_orginal.size  # old_size[0] is in (width, height) format\n",
    "    \n",
    "    ratio = float(desired_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    im_scaled = im_orginal.resize(new_size) # scale up picture before padding to keep information\n",
    "    \n",
    "    delta_w = desired_size - new_size[0]\n",
    "    delta_h = desired_size - new_size[1]\n",
    "    padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
    "    im_padded = ImageOps.expand(im_scaled, padding, fill=255) # padding of scaled picture\n",
    "    \n",
    "    final_size = (40, 40) # input size for CNN\n",
    "    im_final = im_padded.resize(final_size, resample=0) # resize to input size of CNN\n",
    "    return np.array(im_final)\n",
    "\n",
    "def fetch_label(key, lvl='level2'):\n",
    "    return list(labelsDF[labelsDF['objid']==strip(key)][lvl])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create processed images\n",
    "processed_imgs_train = {key : [image_processing(reverse_strip(key)), fetch_label(reverse_strip(key))] for key in train_objids}\n",
    "processed_imgs_val = {key : [image_processing(reverse_strip(key)), fetch_label(reverse_strip(key))] for key in val_objids}\n",
    "processed_imgs_test = {key : [image_processing(reverse_strip(key)), fetch_label(reverse_strip(key))] for key in test_objids}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_object_ids(object_id):\n",
    "    stripped_key = str(object_id)\n",
    "    key_lr = int('8' + stripped_key[1:])\n",
    "    key_ud = int('1' + stripped_key[1:])\n",
    "    key_90 = int('2' + stripped_key[1:])\n",
    "    key_180 = int('4' + stripped_key[1:])\n",
    "    key_270 = int('5' + stripped_key[1:])\n",
    "    key_t1 = int('6' + stripped_key[1:])\n",
    "    key_t2 = int('7' + stripped_key[1:])\n",
    "    return {'lr' : key_lr, 'ud' : key_ud, '90' : key_90, '180' : key_180,\n",
    "                '270' : key_270, 't1' : key_t1, 't2' : key_t2}\n",
    "\n",
    "def generate_duplicates_numpy(object_id, image_files):\n",
    "    # Fetching and making numpy array of original object\n",
    "    img_np_array = image_files[object_id][0]\n",
    "\n",
    "    # Generating copies of data using numpy functions for flipping, rotating and transposing\n",
    "    img_flipped_ud = np.flip(img_np_array, 0)\n",
    "    img_flipped_lr = np.flip(img_np_array, 1)\n",
    "    img_rot_90 = np.rot90(img_np_array, k=1)\n",
    "    img_rot_180 = np.rot90(img_np_array, k=2)\n",
    "    img_rot_270 = np.rot90(img_np_array, k=3)\n",
    "    img_tran_1 = np.transpose(img_np_array)\n",
    "    img_tran_2 = np.flip(np.transpose(np.flip(img_np_array, 1)), 1)\n",
    "    \n",
    "    # Generating new keys\n",
    "    key_dict = generate_new_object_ids(object_id)\n",
    "    # Fetching label\n",
    "    lbl = fetch_label(reverse_strip(object_id))\n",
    "    # Updating dictionary of images with new data\n",
    "    image_files.update({key_dict['ud'] : [img_flipped_ud, lbl], key_dict['lr'] : [img_flipped_lr, lbl], \n",
    "                      key_dict['90'] : [img_rot_90, lbl], key_dict['180'] : [img_rot_180, lbl], \n",
    "                      key_dict['270'] : [img_rot_270, lbl], key_dict['t1'] : [img_tran_1, lbl], \n",
    "                      key_dict['t2'] : [img_tran_2, lbl]})\n",
    "    \n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nr. of new images produced: 60632\n",
      "new length of images: 177938\n"
     ]
    }
   ],
   "source": [
    "# delete old images to free space\n",
    "#del imgFiles\n",
    "\n",
    "def augment_dataset(image_files, list_of_objids):\n",
    "    # To keep track of total amount of images created\n",
    "    total_nr = 0\n",
    "    # We make a new dictionary to keep track of the augmented data, in relation to the original data\n",
    "    #augmented_objid_dict = {}\n",
    "\n",
    "    for label in labels_to_multiply: # tqdm\n",
    "        # Fetch object IDs pertaining to one specific label\n",
    "        object_ids = original_objid_dict[label]\n",
    "\n",
    "        #print('For label:', label[:4], 'we generate', len(object_ids)*8, 'new images')\n",
    "        total_nr += len(object_ids)*8\n",
    "\n",
    "         # Create a list of the object IDs of the newly created images so we can feed them into augmented_objid_dict\n",
    "        object_ids_new = []\n",
    "\n",
    "        # Create new data\n",
    "        for objid in object_ids:\n",
    "            if objid in list_of_objids:\n",
    "                image_files = generate_duplicates_numpy(objid, processed_imgs_train)\n",
    "                list_of_objids += list(generate_new_object_ids(objid).values())\n",
    "\n",
    "        # Append new object IDs to augmented_objid_dict\n",
    "        #augmented_objid_dict.update({label : object_ids + object_ids_new})\n",
    "\n",
    "    #for label in labels_not_to_multiply:\n",
    "        #augmented_objid_dict.update({label : original_objid_dict[label]})\n",
    "    \n",
    "    print('Total nr. of new images produced:', total_nr)\n",
    "    print('new length of images:', len(list(processed_imgs_train.keys())))\n",
    "    return image_files, list_of_objids\n",
    "\n",
    "processed_imgs_train, augmented_objid_train = augment_dataset(processed_imgs_train, train_objids)\n",
    "#nr_keys_in_augmented = 0\n",
    "#for label in list(augmented_objid_dict_train.keys()):\n",
    "#    nr_keys_in_augmented += len(augmented_objid_dict_train[label])\n",
    "\n",
    "#print('Number of object ids in the augmented dataset:', nr_keys_in_augmented)\n",
    "\n",
    "\n",
    "\n",
    "train_objids = random.sample(train_objids, len(train_objids))\n",
    "test_objids = random.sample(test_objids, len(test_objids))\n",
    "val_objids = random.sample(val_objids, len(val_objids))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch pictures and labels, corresponding to keys.\n",
    "# train\n",
    "X_train, y_train = autoloader(augmented_objid_train, processed_imgs_train)\n",
    "# validation\n",
    "X_val, y_val = autoloader(val_objids, processed_imgs_val)\n",
    "# test\n",
    "X_test, y_test = autoloader(test_objids, processed_imgs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Shape Training data-------\n",
      "X_train shape: (177938, 40, 40, 1)\n",
      "y_train shape: (177938, 40)\n",
      "\n",
      "-----Shape Validation data-----\n",
      "X_val shape: (48751, 40, 40, 1)\n",
      "y_val shape: (48751, 40)\n",
      "\n",
      "--------Shape Test data--------\n",
      "X_test shape : (48708, 40, 40, 1)\n",
      "y_test shape : (48708, 40)\n"
     ]
    }
   ],
   "source": [
    "print('-------Shape Training data-------')\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "print('\\n-----Shape Validation data-----')\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "\n",
    "print('\\n--------Shape Test data--------')\n",
    "print('X_test shape :', X_test.shape)\n",
    "print('y_test shape :', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgFiles = extract_zip_to_memory(basepath + \"imgs.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label dataset\n",
    "labelsDF = pd.read_csv(basepath + 'meta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsDF['objid'] = labelsDF['objid'].astype(np.int64, errors='ignore')\n",
    "labelsDF['level1'] = labelsDF['level1'].fillna('No_level1_name')\n",
    "labelsDF['level2'] = labelsDF['level2'].fillna('No_level2_name')\n",
    "\n",
    "print(labelsDF.isnull().sum().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Nr. of unique level1 names:', labelsDF['level1'].nunique())\n",
    "print('Nr. of unique level2 names:', labelsDF['level2'].nunique())\n",
    "print('Nr. of unique names  total:', labelsDF['unique_name'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_unique_names = labelsDF.groupby('level2', as_index=True)['id'].count()\n",
    "number_unique_names.sort_values(inplace=True, ascending=False)\n",
    "\n",
    "sns.set(font_scale = 1.5)\n",
    "figure, ax = plt.subplots(figsize=(23,17))\n",
    "ax = sns.barplot(y=number_unique_names.index, x=np.log10(number_unique_names.values))\n",
    "# ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(),'.2f'), \n",
    "                (p.get_x() + p.get_width() / 2., p.get_width()),\n",
    "                ha = 'center',\n",
    "                va = 'center',\n",
    "                xytext = (0, 10),\n",
    "                textcoords = 'offset points')\n",
    "\n",
    "plt.ylabel('Unique plankton level2 label', fontsize= 16)\n",
    "plt.xlabel('log(Nr. of occurances)', fontsize= 16)\n",
    "plt.title('Exponential occurance of different labels', fontsize=16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the object ids that pertain to specific labels\n",
    "# And fetching the level2 labels of the categories we want to duplicate\n",
    "unique_names = list(number_unique_names.index)\n",
    "original_objid_dict = {}\n",
    "labels_to_multiply = []\n",
    "labels_not_to_multiply = []\n",
    "\n",
    "for name in unique_names:\n",
    "    objid_labels = list(labelsDF.loc[labelsDF['level2'] == name, 'objid'])\n",
    "    original_objid_dict.update({name : objid_labels})\n",
    "    print('Label:', name[:4], 'has', len(objid_labels), 'object IDs.')\n",
    "    # Find labels that we want to generate more data within\n",
    "    if len(objid_labels) < 1000:\n",
    "        labels_to_multiply.append(name)\n",
    "    else:\n",
    "        labels_not_to_multiply.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions used for the autoloader and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip(key):\n",
    "    return int(key[5:-4])\n",
    "\n",
    "def key_to_objid(list_of_keys):\n",
    "    return [strip(key) for key in list_of_keys]\n",
    "\n",
    "def reverse_strip(objid):\n",
    "    return 'imgs/' + str(objid) + '.jpg'\n",
    "\n",
    "def objid_to_key(list_of_objids):\n",
    "    return [reverse_strip(objid) for objid in list_of_objids]\n",
    "\n",
    "# Functions to preprocess images\n",
    "def image_processing(key):\n",
    "    desired_size = 972 # max of width and height in dataset\n",
    "    im_orginal = Image.open(imgFiles[key])\n",
    "    old_size = im_orginal.size  # old_size[0] is in (width, height) format\n",
    "    \n",
    "    ratio = float(desired_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    im_scaled = im_orginal.resize(new_size) # scale up picture before padding to keep information\n",
    "    \n",
    "    delta_w = desired_size - new_size[0]\n",
    "    delta_h = desired_size - new_size[1]\n",
    "    padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
    "    im_padded = ImageOps.expand(im_scaled, padding, fill=255) # padding of scaled picture\n",
    "    \n",
    "    final_size = (40, 40) # input size for CNN\n",
    "    im_final = im_padded.resize(final_size, resample=0) # resize to input size of CNN\n",
    "    return np.array(im_final)\n",
    "\n",
    "def fetch_label(key, lvl='level2'):\n",
    "    return list(labelsDF[labelsDF['objid']==strip(key)][lvl])[0]\n",
    "\n",
    "#print(fetch_label('imgs/32627324.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_keys = list(imgFiles.keys())\n",
    "processed_imgs = {strip(key) : [image_processing(key), fetch_label(key)] for key in tqdm(img_keys)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del imgFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_object_ids(object_id):\n",
    "    stripped_key = str(object_id)\n",
    "    key_lr = int('8' + stripped_key[1:])\n",
    "    key_ud = int('1' + stripped_key[1:])\n",
    "    key_90 = int('2' + stripped_key[1:])\n",
    "    key_180 = int('4' + stripped_key[1:])\n",
    "    key_270 = int('5' + stripped_key[1:])\n",
    "    key_t1 = int('6' + stripped_key[1:])\n",
    "    key_t2 = int('7' + stripped_key[1:])\n",
    "    return {'lr' : key_lr, 'ud' : key_ud, '90' : key_90, '180' : key_180,\n",
    "                '270' : key_270, 't1' : key_t1, 't2' : key_t2}\n",
    "\n",
    "def generate_duplicates_numpy(object_id, image_files):\n",
    "    # Fetching and making numpy array of original object\n",
    "    img_np_array = image_files[object_id][0]\n",
    "\n",
    "    # Generating copies of data using numpy functions for flipping, rotating and transposing\n",
    "    img_flipped_ud = np.flip(img_np_array, 0)\n",
    "    img_flipped_lr = np.flip(img_np_array, 1)\n",
    "    img_rot_90 = np.rot90(img_np_array, k=1)\n",
    "    img_rot_180 = np.rot90(img_np_array, k=2)\n",
    "    img_rot_270 = np.rot90(img_np_array, k=3)\n",
    "    img_tran_1 = np.transpose(img_np_array)\n",
    "    img_tran_2 = np.flip(np.transpose(np.flip(img_np_array, 1)), 1)\n",
    "    \n",
    "    # Generating new keys\n",
    "    key_dict = generate_new_object_ids(object_id)\n",
    "    # Fetching label\n",
    "    lbl = fetch_label(reverse_strip(object_id))\n",
    "    # Updating dictionary of images with new data\n",
    "    image_files.update({key_dict['ud'] : [img_flipped_ud, lbl], key_dict['lr'] : [img_flipped_lr, lbl], \n",
    "                      key_dict['90'] : [img_rot_90, lbl], key_dict['180'] : [img_rot_180, lbl], \n",
    "                      key_dict['270'] : [img_rot_270, lbl], key_dict['t1'] : [img_tran_1, lbl], \n",
    "                      key_dict['t2'] : [img_tran_2, lbl]})\n",
    "    \n",
    "    return image_files\n",
    "\n",
    "# Maybe worth checking out for keras data generation later \n",
    "# https://medium.com/@ksusorokina/image-classification-with-convolutional-neural-networks-496815db12a8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep track of total amount of images created\n",
    "total_nr = 0\n",
    "# We make a new dictionary to keep track of the augmented data, in relation to the original data\n",
    "augmented_objid_dict = {}\n",
    "\n",
    "for label in labels_to_multiply:\n",
    "    # Fetch object IDs pertaining to one specific label\n",
    "    object_ids = original_objid_dict[label]\n",
    "    \n",
    "    #print('For label:', label[:4], 'we generate', len(object_ids)*8, 'new images')\n",
    "    total_nr += len(object_ids)*8\n",
    "    \n",
    "     # Create a list of the object IDs of the newly created images so we can feed them into augmented_objid_dict\n",
    "    object_ids_new = []\n",
    "    \n",
    "    # Create new data\n",
    "    for objid in object_ids:\n",
    "        processed_imgs = generate_duplicates_numpy(objid)\n",
    "        object_ids_new += (list(generate_new_object_ids(objid).values()))\n",
    "    \n",
    "    # Append new object IDs to augmented_objid_dict\n",
    "    augmented_objid_dict.update({label : object_ids + object_ids_new})\n",
    "    \n",
    "for label in labels_not_to_multiply:\n",
    "    augmented_objid_dict.update({label : original_objid_dict[label]})\n",
    "print('Total nr of new images produced:', total_nr)\n",
    "print('new length of images:', len(list(processed_imgs.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing data into test and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(dict_objids_and_labels):\n",
    "    train_objids = []\n",
    "    test_objids = [] \n",
    "    val_objids = []\n",
    "    for label in unique_names: # unique names are all labels in level 2 of labelsDF\n",
    "        object_ids = dict_objids_and_labels[label]\n",
    "        number_train = int(len(object_ids)*0.6)\n",
    "        number_test = int(len(object_ids)*0.2)\n",
    "        train_objids += object_ids[:number_train]\n",
    "        test_objids += object_ids[number_train:(number_train + number_test)]\n",
    "        val_objids += object_ids[(number_train + number_test):]\n",
    "    return train_objids, test_objids, val_objids\n",
    "\n",
    "# original_objid_dict\n",
    "# augmented_objid_dict\n",
    "train_objids, test_objids, val_objids = train_test_val_split(augmented_objid_dict)\n",
    "\n",
    "train_objids = random.sample(train_objids, len(train_objids))\n",
    "test_objids = random.sample(test_objids, len(test_objids))\n",
    "val_objids = random.sample(val_objids, len(val_objids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train length: ', len(train_objids))\n",
    "print('Test length: ', len(test_objids))\n",
    "print('Val length: ', len(val_objids))\n",
    "print(len(train_objids) + len(test_objids) + len(val_objids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to label-encode the different categories so we can one-hot encode them.  \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def autoloader(list_of_object_ids):\n",
    "    X = np.array([processed_imgs[object_id][0] for object_id in list_of_object_ids])\n",
    "    y = np.array([processed_imgs[object_id][1] for object_id in list_of_object_ids])\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    # one-hot encode target column\n",
    "    y = to_categorical(y)\n",
    "\n",
    "    # Reshape input data to account for images only being greyscale\n",
    "    X = X.reshape(len(list_of_object_ids), 40, 40, 1)\n",
    "    return X, y\n",
    "\n",
    "# Fetch pictures and labels, corresponding to keys.\n",
    "# train\n",
    "X_train, y_train = autoloader(train_objids)\n",
    "# test\n",
    "X_test, y_test = autoloader(test_objids)\n",
    "# val\n",
    "X_val, y_val = autoloader(val_objids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('y_val shape:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for keras CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# input dimensions\n",
    "input_shape = (40, 40, 1)\n",
    "# network parameters \n",
    "batch_size = 128\n",
    "classes = labelsDF.level2.unique()\n",
    "num_classes = labelsDF.level2.nunique()\n",
    "epochs = 1 # Further Fine Tuning can be done\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architectures\n",
    " \n",
    "- input -> (conv -> pool)*2 -> fc -> softmax\n",
    "- input -> (conv -> conv -> pool)*3-4 -> fc -> fc -> softmax\n",
    "\n",
    "#### To further explore\n",
    "Data augementation of pictures with rare labels\n",
    "- https://medium.com/@arindambaidya168/https-medium-com-arindambaidya168-using-keras-imagedatagenerator-b94a87cdefad\n",
    "- https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 38, 38, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 36, 36, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 18, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               401536    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40)                5160      \n",
      "=================================================================\n",
      "Total params: 471,688\n",
      "Trainable params: 471,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Keras CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# add first  layer\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add second layer\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add third layer\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "#model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "# add flatten layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "# add last layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# complile the model and view its architecur\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,  optimizer=\"Adam\", metrics=['accuracy', f1])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 177938 samples, validate on 48751 samples\n",
      "Epoch 1/1\n",
      "177938/177938 [==============================] - 701s 4ms/step - loss: 4.6535 - acc: 0.4947 - f1: 0.0351 - val_loss: 1.0868 - val_acc: 0.6947 - val_f1: 0.0814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8cba0a0390>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import f1_score\n",
    "test_predict = model.predict(X_test)\n",
    "test_f1 = f1(y_test, test_predict)\n",
    "\n",
    "print(\"Test F1 Score: \", K.get_value(test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(Conv32 -> MaxPool) -> (Conv64 -> MaxPool) -> Flatten -> Dense128 -> Dropout0.5 -> DenseNumClasses\n",
    "Test F1 Score: 0.09016953\n",
    "\n",
    "(2*Conv32 -> MaxPool) -> (2*Conv64 -> MaxPool) -> Flatten -> Dense128 -> Dropout0.5 -> DenseNumClasses\n",
    "Test F1 Score:  0.10577502\n",
    "\n",
    "(Conv32 -> MaxPool) -> (Conv64 -> MaxPool) -> (Conv128 -> MaxPool) -> Flatten -> Dense128 -> Dropout0.5 -> DenseNumClasses\n",
    "Test F1 Score: 0.058656134\n",
    "\n",
    "(Conv32 -> MaxPool) -> BatchNormalization -> (Conv64 -> MaxPool) -> Flatten -> Dense128 -> Dropout0.5 -> DenseNumClasses\n",
    "Test F1 Score: 0.04565934"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
